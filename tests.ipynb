{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "import pathlib\n",
    "import music21\n",
    "from music21 import converter, instrument, note, chord\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "import music21.note\n",
    "from music21 import stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "# folder = pathlib.Path('maestro-v3.0.0-midi/maestro-v3.0.0/2018')\n",
    "# filenames = glob.glob(str(folder/'*.midi'))\n",
    "# print(len(filenames))\n",
    "\n",
    "songs = []\n",
    "folder = Path('maestro-v3.0.0-midi/maestro-v3.0.0/2018')\n",
    "for file in folder.rglob('*.midi'):\n",
    "  songs.append(file)\n",
    "print(len(songs))\n",
    "\n",
    "import random\n",
    "# Get a subset of 1000 songs\n",
    "result =  random.sample([x for x in songs], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital9-11_MID--AUDIO_11_R1_2018_wav--1.midi\n",
      "2: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital17-19_MID--AUDIO_19_R1_2018_wav--3.midi\n",
      "3: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital1-3_MID--AUDIO_03_R1_2018_wav--2.midi\n",
      "4: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital9-11_MID--AUDIO_11_R1_2018_wav--2.midi\n",
      "5: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Chamber6_MID--AUDIO_20_R3_2018_wav--1.midi\n",
      "6: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--2.midi\n",
      "7: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital17-19_MID--AUDIO_17_R1_2018_wav--3.midi\n",
      "8: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital5-7_MID--AUDIO_06_R1_2018_wav--1.midi\n",
      "9: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital5-7_MID--AUDIO_07_R1_2018_wav--4.midi\n",
      "10: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Schubert1-3_MID--AUDIO_05_R2_2018_wav.midi\n"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "for i,file in enumerate(result):\n",
    "    print(f'{i+1}: {file}')\n",
    "    try:\n",
    "      midi = converter.parse(file)\n",
    "      notes_to_parse = None\n",
    "      parts = instrument.partitionByInstrument(midi)\n",
    "      if parts: # file has instrument parts\n",
    "          notes_to_parse = parts.parts[0].recurse()\n",
    "      else: # file has notes in a flat structure\n",
    "          notes_to_parse = midi.flat.notes\n",
    "      for element in notes_to_parse:\n",
    "          if isinstance(element, note.Note):\n",
    "              notes.append(str(element.pitch))\n",
    "          elif isinstance(element, chord.Chord):\n",
    "              notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    except:\n",
    "      print(f'FAILED: {i+1}: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('notes', 'wb') as filepath:\n",
    "  pickle.dump(notes, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55645\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 32\n",
    "\n",
    "    # Get all unique pitchnames\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    numPitches = len(pitchnames)\n",
    "\n",
    "     # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        # sequence_in is a sequence_length list containing sequence_length notes\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        # sequence_out is the sequence_length + 1 note that comes after all the notes in\n",
    "        # sequence_in. This is so the model can read sequence_length notes before predicting\n",
    "        # the next one.\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        # network_input is the same as sequence_in but it containes the indexes from the notes\n",
    "        # because the model is only fed the indexes.\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        # network_output containes the index of the sequence_out\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    # n_patters is the length of the times it was iterated \n",
    "    # for example if i = 3, then n_patterns = 3\n",
    "    # because network_input is a list of lists\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    # Reshapes it into a n_patterns by sequence_length matrix\n",
    "    print(len(network_input))\n",
    "    \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    # OneHot encodes the network_output\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "\n",
    "n_vocab = len(set(notes))\n",
    "network_input, network_output = prepare_sequences(notes,n_vocab)\n",
    "n_patterns = len(network_input)\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "numPitches = len(pitchnames)\n",
    "\n",
    "#print(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def oversample(network_input,network_output,sequence_length=15):\n",
    "\n",
    "#   n_patterns = len(network_input)\n",
    "#   # Create a DataFrame from the two matrices\n",
    "#   new_df = pd.concat([pd.DataFrame(network_input),pd.DataFrame(network_output)],axis=1)\n",
    "\n",
    "#   # Rename the columns to numbers and Notes\n",
    "#   new_df.columns = [x for x in range(sequence_length+1)]\n",
    "#   new_df = new_df.rename(columns={sequence_length:'Notes'})\n",
    "\n",
    "#   print(new_df.tail(20))\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the preoversampled DataFrame: {new_df[\"Notes\"].value_counts()}')\n",
    "#   # Oversampling\n",
    "#   oversampled_df = new_df.copy()\n",
    "#   #max_class_size = np.max(oversampled_df['Notes'].value_counts())\n",
    "#   max_class_size = 700\n",
    "#   print('Size of biggest class: ', max_class_size)\n",
    "\n",
    "#   class_subsets = [oversampled_df.query('Notes == ' + str(i)) for i in range(len(new_df[\"Notes\"].unique()))] # range(2) because it is a binary class\n",
    "\n",
    "#   for i in range(len(new_df['Notes'].unique())):\n",
    "#     try:\n",
    "#       class_subsets[i] = class_subsets[i].sample(max_class_size,random_state=42,replace=True)\n",
    "#     except:\n",
    "#       print(i)\n",
    "\n",
    "#   oversampled_df = pd.concat(class_subsets,axis=0).sample(frac=1.0,random_state=42).reset_index(drop=True)\n",
    "\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the oversampled DataFrame: {oversampled_df[\"Notes\"].value_counts()}')\n",
    "\n",
    "#   # Get a sample from the oversampled DataFrame (because it may be too big, and we also have to convert it into a 3D array for the LSTM)\n",
    "#   sampled_df = oversampled_df.sample(n_patterns,replace=True) # 99968*32 has to be equals to (99968,32,1)\n",
    "\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the oversampled post-sampled DataFrame: {sampled_df[\"Notes\"].value_counts()}')\n",
    "\n",
    "#   # Convert the training columns back to a 3D array\n",
    "#   network_in = sampled_df[[x for x in range(sequence_length)]]\n",
    "#   network_in = np.array(network_in)\n",
    "#   network_in = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "#   network_in = network_in / numPitches\n",
    "#   print(network_in.shape)\n",
    "#   print(sampled_df['Notes'].shape)\n",
    "#   # Converts the target column into a OneHot encoded matrix\n",
    "#   network_out = pd.get_dummies(sampled_df['Notes'])\n",
    "#   print(network_out.shape)\n",
    "\n",
    "#   return network_in,network_out\n",
    "\n",
    "# networkInputShaped,networkOutputShaped = oversample(network_input,network_output,sequence_length=15)\n",
    "# networkOutputShaped = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(256))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dense(256))\n",
    "model.add(LSTM(512))\n",
    "#model.add(Dense(numPitches))\n",
    "model.add(Dense(numPitches))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "870/870 [==============================] - 1168s 1s/step - loss: 5.2457 - accuracy: 0.0168\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.24572, saving model to weights-improvement-01-5.2457-bigger_1.hdf5\n",
      "Epoch 2/2\n",
      "870/870 [==============================] - 1165s 1s/step - loss: 5.2024 - accuracy: 0.0179\n",
      "\n",
      "Epoch 00002: loss improved from 5.24572 to 5.20243, saving model to weights-improvement-02-5.2024-bigger_1.hdf5\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger_1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=1,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(network_input, network_output, epochs=num_epochs, batch_size=64, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 35566\n"
     ]
    }
   ],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    # Selects a random row from the network_input\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    print(f'start: {start}')\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    # Random row from network_input\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        # Reshapes pattern into a vector\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        # Standarizes pattern\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        # Predicts the next note\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        # Outputs a OneHot encoded vector, so this picks the columns\n",
    "        # with the highest probability\n",
    "        index = np.argmax(prediction)\n",
    "        # Maps the note to its respective index\n",
    "        result = int_to_note[index]\n",
    "        # Appends the note to the prediction_output\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        # Adds the predicted note to the pattern\n",
    "        pattern = np.append(pattern,index)\n",
    "        # Slices the array so that it contains the predicted note\n",
    "        # eliminating the first from the array, so the model can\n",
    "        # have a sequence\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "\n",
    "sequence_length = 32\n",
    "\n",
    "n_vocab = len(set(notes))\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = ['B2', 'B2', '2.7', '5.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\n",
    "prediction = model.predict(prediction_input, verbose=0)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='test.mid')\n",
    "create_midi(prediction_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582, 128)\n",
      "(703, 128)\n"
     ]
    }
   ],
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "import numpy as np\n",
    "import os \n",
    "import pretty_midi\n",
    "\n",
    "def midi_to_array(midi_path):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "    \n",
    "    # Get the piano roll representation\n",
    "    piano_roll = midi_data.get_piano_roll(fs=1)  # Set fs to control time resolution\n",
    "    \n",
    "    # Transpose the piano roll to have time steps as rows and pitches as columns\n",
    "    piano_roll = piano_roll.T\n",
    "    \n",
    "    return piano_roll\n",
    "\n",
    "# def midi_to_notes(midi_file, length = 1000):\n",
    "#     mid = MidiFile(midi_file)\n",
    "#     notes = []\n",
    "#     for msg in mid:\n",
    "#         if not msg.is_meta and msg.channel == 0 and msg.type == \"note_on\":\n",
    "#             data = msg.bytes()\n",
    "#             if data[2] != 0:\n",
    "#                 notes.append(data[1])\n",
    "#     notes = np.array(notes)\n",
    "#     notes = notes[:length]\n",
    "#     return notes\n",
    "\n",
    "# Example usage\n",
    "midi_path = 'maestro-v3.0.0-midi/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.midi'\n",
    "midi_pathg = \"maestro-v3.0.0-midi/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi\"\n",
    "midi_array = midi_to_array(midi_path)\n",
    "midi_arrayg = midi_to_array(midi_pathg)\n",
    "print(midi_array.shape)  # Print the shape of the resulting numpy array\n",
    "print(midi_arrayg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir(\"maestro-v3.0.0-midi/maestro-v3.0.0/2018\"):\n",
    "    if file.endswith(\".midi\"):\n",
    "        data.append(midi_to_array(\"maestro-v3.0.0-midi/maestro-v3.0.0/2018/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pad the data\n",
    "max_len = max([x.shape[0] for x in data])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = []\n",
    "for x in data:\n",
    "    pad = np.zeros((max_len - x.shape[0], x.shape[1]))\n",
    "    padded_data.append(np.concatenate((x, pad), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2562, 128)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver padded data to midi files\n",
    "padded_data[0] = padded_data[0].T\n",
    "padded_data[0].shape\n",
    "\n",
    "#convert to midi\n",
    "def array_to_midi(array, name):\n",
    "    array = array.T\n",
    "    midi = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    midi.tracks.append(track)\n",
    "    for i in range(array.shape[0]):\n",
    "        if array[i, 0] != 0:\n",
    "            track.append(Message('note_on', note=int(array[i, 0]), velocity=int(array[i, 1]), time=int(array[i, 2])))\n",
    "    midi.save(name)\n",
    "\n",
    "array_to_midi(data[0], \"test.midi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        batch = batch.transpose(0, 1)\n",
    "        return batch\n",
    "\n",
    "dataset = MidiDataset(padded_data, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1000, batch_size, self.hidden_size), torch.zeros(1000, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disciminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(1, 64, 2, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator(input_size, hidden_size, output_size)\n",
    "        self.discriminator = Disciminator()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.generator(x, hidden)\n",
    "        x = self.discriminator(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.generator.init_hidden(batch_size)\n",
    "\n",
    "def train(dataloader, model, optimizer, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch_size = batch.shape[1]\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            for j in range(batch.shape[0]):\n",
    "                optimizer.zero_grad()\n",
    "                x = batch[j].unsqueeze(0).unsqueeze(2).float()\n",
    "                y = torch.ones(batch_size, 1)\n",
    "                y_hat, hidden = model(x, hidden)\n",
    "                loss = criterion(y_hat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmusic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
