{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "import pathlib\n",
    "import music21\n",
    "from music21 import converter, instrument, note, chord\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = pathlib.Path('maestro-v3.0.0-midi/maestro-v3.0.0/2018')\n",
    "# filenames = glob.glob(str(folder/'*.midi'))\n",
    "# print(len(filenames))\n",
    "\n",
    "songs = []\n",
    "folder = Path('maestro-v3.0.0-midi/maestro-v3.0.0/2018')\n",
    "for file in folder.rglob('*.midi'):\n",
    "  songs.append(file)\n",
    "print(len(songs))\n",
    "\n",
    "import random\n",
    "# Get a subset of 1000 songs\n",
    "result =  random.sample([x for x in songs], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital17-19_MID--AUDIO_17_R1_2018_wav--4.midi\n",
      "2: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital17-19_MID--AUDIO_19_R1_2018_wav--2.midi\n",
      "3: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital12_MID--AUDIO_12_R1_2018_wav--3.midi\n",
      "4: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital5-7_MID--AUDIO_07_R1_2018_wav--2.midi\n",
      "5: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital20_MID--AUDIO_20_R1_2018_wav--4.midi\n",
      "6: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Schubert10-12_MID--AUDIO_17_R2_2018_wav.midi\n",
      "7: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital9-11_MID--AUDIO_09_R1_2018_wav--3.midi\n",
      "8: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Chamber6_MID--AUDIO_20_R3_2018_wav--2.midi\n",
      "9: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.midi\n",
      "10: maestro-v3.0.0-midi\\maestro-v3.0.0\\2018\\MIDI-Unprocessed_Recital9-11_MID--AUDIO_09_R1_2018_wav--4.midi\n"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "for i,file in enumerate(result):\n",
    "    print(f'{i+1}: {file}')\n",
    "    try:\n",
    "      midi = converter.parse(file)\n",
    "      notes_to_parse = None\n",
    "      parts = instrument.partitionByInstrument(midi)\n",
    "      if parts: # file has instrument parts\n",
    "          notes_to_parse = parts.parts[0].recurse()\n",
    "      else: # file has notes in a flat structure\n",
    "          notes_to_parse = midi.flat.notes\n",
    "      for element in notes_to_parse:\n",
    "          if isinstance(element, note.Note):\n",
    "              notes.append(str(element.pitch))\n",
    "          elif isinstance(element, chord.Chord):\n",
    "              notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    except:\n",
    "      print(f'FAILED: {i+1}: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('notes', 'wb') as filepath:\n",
    "  pickle.dump(notes, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56358\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 32\n",
    "\n",
    "    # Get all unique pitchnames\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    numPitches = len(pitchnames)\n",
    "\n",
    "     # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        # sequence_in is a sequence_length list containing sequence_length notes\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        # sequence_out is the sequence_length + 1 note that comes after all the notes in\n",
    "        # sequence_in. This is so the model can read sequence_length notes before predicting\n",
    "        # the next one.\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        # network_input is the same as sequence_in but it containes the indexes from the notes\n",
    "        # because the model is only fed the indexes.\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        # network_output containes the index of the sequence_out\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    # n_patters is the length of the times it was iterated \n",
    "    # for example if i = 3, then n_patterns = 3\n",
    "    # because network_input is a list of lists\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    # Reshapes it into a n_patterns by sequence_length matrix\n",
    "    print(len(network_input))\n",
    "    \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    # OneHot encodes the network_output\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "\n",
    "n_vocab = len(set(notes))\n",
    "network_input, network_output = prepare_sequences(notes,n_vocab)\n",
    "n_patterns = len(network_input)\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "numPitches = len(pitchnames)\n",
    "\n",
    "#print(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def oversample(network_input,network_output,sequence_length=15):\n",
    "\n",
    "#   n_patterns = len(network_input)\n",
    "#   # Create a DataFrame from the two matrices\n",
    "#   new_df = pd.concat([pd.DataFrame(network_input),pd.DataFrame(network_output)],axis=1)\n",
    "\n",
    "#   # Rename the columns to numbers and Notes\n",
    "#   new_df.columns = [x for x in range(sequence_length+1)]\n",
    "#   new_df = new_df.rename(columns={sequence_length:'Notes'})\n",
    "\n",
    "#   print(new_df.tail(20))\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the preoversampled DataFrame: {new_df[\"Notes\"].value_counts()}')\n",
    "#   # Oversampling\n",
    "#   oversampled_df = new_df.copy()\n",
    "#   #max_class_size = np.max(oversampled_df['Notes'].value_counts())\n",
    "#   max_class_size = 700\n",
    "#   print('Size of biggest class: ', max_class_size)\n",
    "\n",
    "#   class_subsets = [oversampled_df.query('Notes == ' + str(i)) for i in range(len(new_df[\"Notes\"].unique()))] # range(2) because it is a binary class\n",
    "\n",
    "#   for i in range(len(new_df['Notes'].unique())):\n",
    "#     try:\n",
    "#       class_subsets[i] = class_subsets[i].sample(max_class_size,random_state=42,replace=True)\n",
    "#     except:\n",
    "#       print(i)\n",
    "\n",
    "#   oversampled_df = pd.concat(class_subsets,axis=0).sample(frac=1.0,random_state=42).reset_index(drop=True)\n",
    "\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the oversampled DataFrame: {oversampled_df[\"Notes\"].value_counts()}')\n",
    "\n",
    "#   # Get a sample from the oversampled DataFrame (because it may be too big, and we also have to convert it into a 3D array for the LSTM)\n",
    "#   sampled_df = oversampled_df.sample(n_patterns,replace=True) # 99968*32 has to be equals to (99968,32,1)\n",
    "\n",
    "#   print('###################################################')\n",
    "#   print(f'Distribution of notes in the oversampled post-sampled DataFrame: {sampled_df[\"Notes\"].value_counts()}')\n",
    "\n",
    "#   # Convert the training columns back to a 3D array\n",
    "#   network_in = sampled_df[[x for x in range(sequence_length)]]\n",
    "#   network_in = np.array(network_in)\n",
    "#   network_in = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "#   network_in = network_in / numPitches\n",
    "#   print(network_in.shape)\n",
    "#   print(sampled_df['Notes'].shape)\n",
    "#   # Converts the target column into a OneHot encoded matrix\n",
    "#   network_out = pd.get_dummies(sampled_df['Notes'])\n",
    "#   print(network_out.shape)\n",
    "\n",
    "#   return network_in,network_out\n",
    "\n",
    "# networkInputShaped,networkOutputShaped = oversample(network_input,network_output,sequence_length=15)\n",
    "# networkOutputShaped = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(256))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dense(256))\n",
    "model.add(LSTM(512))\n",
    "#model.add(Dense(numPitches))\n",
    "model.add(Dense(numPitches))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 14/881 [..............................] - ETA: 16:36 - loss: 5.3628 - accuracy: 0.0123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m      7\u001b[0m     filepath, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,        \n\u001b[0;32m      9\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,        \n\u001b[0;32m     10\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m )    \n\u001b[0;32m     12\u001b[0m callbacks_list \u001b[39m=\u001b[39m [checkpoint]\n\u001b[1;32m---> 14\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(network_input, network_output, epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks_list)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1187\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1181\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1182\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1183\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1184\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1185\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1186\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1187\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1188\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1189\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger_1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=1,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(network_input, network_output, epochs=num_epochs, batch_size=64, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21.note import allNotes\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    # Selects a random row from the network_input\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    print(f'start: {start}')\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    # Random row from network_input\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        # Reshapes pattern into a vector\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        # Standarizes pattern\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        # Predicts the next note\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        # Outputs a OneHot encoded vector, so this picks the columns\n",
    "        # with the highest probability\n",
    "        index = np.argmax(prediction)\n",
    "        # Maps the note to its respective index\n",
    "        result = int_to_note[index]\n",
    "        # Appends the note to the prediction_output\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        # Adds the predicted note to the pattern\n",
    "        pattern = np.append(pattern,index)\n",
    "        # Slices the array so that it contains the predicted note\n",
    "        # eliminating the first from the array, so the model can\n",
    "        # have a sequence\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "\n",
    "n_vocab = len(set(allNotes))\n",
    "pitchnames = sorted(set(item for item in allNotes))\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='test.mid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582, 128)\n",
      "(703, 128)\n"
     ]
    }
   ],
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "import numpy as np\n",
    "import os \n",
    "import pretty_midi\n",
    "\n",
    "def midi_to_array(midi_path):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "    \n",
    "    # Get the piano roll representation\n",
    "    piano_roll = midi_data.get_piano_roll(fs=1)  # Set fs to control time resolution\n",
    "    \n",
    "    # Transpose the piano roll to have time steps as rows and pitches as columns\n",
    "    piano_roll = piano_roll.T\n",
    "    \n",
    "    return piano_roll\n",
    "\n",
    "# def midi_to_notes(midi_file, length = 1000):\n",
    "#     mid = MidiFile(midi_file)\n",
    "#     notes = []\n",
    "#     for msg in mid:\n",
    "#         if not msg.is_meta and msg.channel == 0 and msg.type == \"note_on\":\n",
    "#             data = msg.bytes()\n",
    "#             if data[2] != 0:\n",
    "#                 notes.append(data[1])\n",
    "#     notes = np.array(notes)\n",
    "#     notes = notes[:length]\n",
    "#     return notes\n",
    "\n",
    "# Example usage\n",
    "midi_path = 'maestro-v3.0.0-midi/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.midi'\n",
    "midi_pathg = \"maestro-v3.0.0-midi/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi\"\n",
    "midi_array = midi_to_array(midi_path)\n",
    "midi_arrayg = midi_to_array(midi_pathg)\n",
    "print(midi_array.shape)  # Print the shape of the resulting numpy array\n",
    "print(midi_arrayg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir(\"maestro-v3.0.0-midi/maestro-v3.0.0/2018\"):\n",
    "    if file.endswith(\".midi\"):\n",
    "        data.append(midi_to_array(\"maestro-v3.0.0-midi/maestro-v3.0.0/2018/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pad the data\n",
    "max_len = max([x.shape[0] for x in data])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = []\n",
    "for x in data:\n",
    "    pad = np.zeros((max_len - x.shape[0], x.shape[1]))\n",
    "    padded_data.append(np.concatenate((x, pad), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2562, 128)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver padded data to midi files\n",
    "padded_data[0] = padded_data[0].T\n",
    "padded_data[0].shape\n",
    "\n",
    "#convert to midi\n",
    "def array_to_midi(array, name):\n",
    "    array = array.T\n",
    "    midi = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    midi.tracks.append(track)\n",
    "    for i in range(array.shape[0]):\n",
    "        if array[i, 0] != 0:\n",
    "            track.append(Message('note_on', note=int(array[i, 0]), velocity=int(array[i, 1]), time=int(array[i, 2])))\n",
    "    midi.save(name)\n",
    "\n",
    "array_to_midi(data[0], \"test.midi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        batch = batch.transpose(0, 1)\n",
    "        return batch\n",
    "\n",
    "dataset = MidiDataset(padded_data, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1000, batch_size, self.hidden_size), torch.zeros(1000, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disciminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(1, 64, 2, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator(input_size, hidden_size, output_size)\n",
    "        self.discriminator = Disciminator()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.generator(x, hidden)\n",
    "        x = self.discriminator(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.generator.init_hidden(batch_size)\n",
    "\n",
    "def train(dataloader, model, optimizer, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch_size = batch.shape[1]\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            for j in range(batch.shape[0]):\n",
    "                optimizer.zero_grad()\n",
    "                x = batch[j].unsqueeze(0).unsqueeze(2).float()\n",
    "                y = torch.ones(batch_size, 1)\n",
    "                y_hat, hidden = model(x, hidden)\n",
    "                loss = criterion(y_hat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmusic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
